# this is for grpc client
services:
  triton-server:
    build:
      context: ..
      dockerfile: deployment/Dockerfile
    container_name: phone-detection

    # GPU runtime for TensorRT MLP model (CPU preprocessing + GPU TensorRT MLP)
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=0

    ports:
      - "8010:8000"    # HTTP endpoint
      - "8011:8001"    # gRPC endpoint
      - "8012:8002"    # Metrics endpoint

    volumes:
      - ../models/model_repository:/models:ro
      - ./logs:/logs

    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/v2/health/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

    restart: unless-stopped

  triton-client:
    build:
      context: .
      dockerfile: Dockerfile.client
    container_name: triton-client
    depends_on:
      - triton-server
    volumes:
      - ..:/workspace
    tty: true
    stdin_open: true
    command: /bin/bash

    # Enable GPU support for client testing if needed
    runtime: nvidia
    environment:
      - NVIDIA_VISIBLE_DEVICES=all

    restart: "no"
